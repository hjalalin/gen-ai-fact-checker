{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10055796,"sourceType":"datasetVersion","datasetId":6196261}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport time\nimport os\nimport requests\nimport xml.etree.ElementTree as ET\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom IPython.display import Markdown, display, YouTubeVideo\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom google.auth.credentials import AnonymousCredentials\nfrom google.auth import compute_engine\nfrom IPython.display import Markdown, display\nfrom kaggle_secrets import UserSecretsClient\nfrom nltk.corpus import wordnet\n\n! pip install -q youtube-search-python \n!pip install youtube-transcript-api \nfrom youtubesearchpython import VideosSearch\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom youtube_transcript_api.formatters import JSONFormatter","metadata":{"_uuid":"40e0500a-3904-4173-b4a5-d7976be33f68","_cell_guid":"36457369-65b6-4d8b-9052-6c5f5e06e075","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:28:46.028972Z","iopub.execute_input":"2024-11-30T21:28:46.029349Z","iopub.status.idle":"2024-11-30T21:29:13.525408Z","shell.execute_reply.started":"2024-11-30T21:28:46.029310Z","shell.execute_reply":"2024-11-30T21:29:13.524308Z"}},"outputs":[{"name":"stdout","text":"Collecting youtube-transcript-api\n  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (0.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2024.8.30)\nDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: youtube-transcript-api\nSuccessfully installed youtube-transcript-api-0.6.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def mock_get_universe_domain(request):\n    return \"googleapis.com\"\n\n# Override the original metadata fetching function\ncompute_engine._metadata.get_universe_domain = mock_get_universe_domain\n\n# Set the Google application credentials manually (replace with the actual path)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/google-cloud-key/esoteric-cab-443306-n6-8b6ccf376c34.json\"","metadata":{"_uuid":"1b9f114b-9313-4821-a216-d24aa7ae7890","_cell_guid":"5b1831eb-9266-487e-b77b-7b0fabc8e1f0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.527555Z","iopub.execute_input":"2024-11-30T21:29:13.528404Z","iopub.status.idle":"2024-11-30T21:29:13.534094Z","shell.execute_reply.started":"2024-11-30T21:29:13.528364Z","shell.execute_reply":"2024-11-30T21:29:13.532986Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GEMINI-API-KEY\")\nsecret_value_1 = user_secrets.get_secret(\"ncbi_api_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:29:13.535700Z","iopub.execute_input":"2024-11-30T21:29:13.536149Z","iopub.status.idle":"2024-11-30T21:29:13.849803Z","shell.execute_reply.started":"2024-11-30T21:29:13.536101Z","shell.execute_reply":"2024-11-30T21:29:13.848755Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def fetch_pmid_list(query, max_results=100):\n    base_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key={secret_value_1}\"\n    params = {\n        \"db\": \"pubmed\",\n        \"term\": query,\n        \"retmax\": max_results,\n        \"usehistory\": \"y\"\n    }\n    response = requests.get(base_url, params=params)\n    time.sleep(1)\n    if response.status_code == 200:\n        root = ET.fromstring(response.content)\n        webenv = root.find(\"WebEnv\").text\n        query_key = root.find(\"QueryKey\").text\n        pmids = [id.text for id in root.findall(\"IdList/Id\")]\n        return pmids, webenv, query_key\n    else:\n        print(\"Error fetching PMIDs.\")\n        return [], None, None\n        \n\ndef fetch_article_details(pmids, webenv, query_key, retstart=0, retmax=100):\n    base_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key={secret_value_1}\"\n    ids = \",\".join(pmids)\n    params = {\n        \"db\": \"pubmed\",\n        \"id\": ids,\n        \"retstart\": retstart,\n        \"retmax\": retmax,\n        \"WebEnv\": webenv,\n        \"query_key\": query_key,\n        \"rettype\": \"xml\",\n        \"retmode\": \"xml\"\n    }\n    response = requests.get(base_url, params=params)\n    time.sleep(1)\n    if response.status_code == 200:\n        root = ET.fromstring(response.content)\n        articles = []\n        for docsum in root.findall(\"PubmedArticle\"):\n            article = {}\n            medline_citation = docsum.find(\"MedlineCitation\")\n            if medline_citation is not None:\n                article[\"pmid\"] = medline_citation.find(\"PMID\").text\n                article[\"title\"] = medline_citation.find(\"Article/ArticleTitle\").text\n                article[\"source\"] = medline_citation.find(\"Article/Journal/Title\").text\n                article[\"authors\"] = []\n                for author in medline_citation.findall(\"Article/AuthorList/Author\"):\n                    last_name = author.find(\"LastName\")\n                    fore_name = author.find(\"ForeName\")\n                    if last_name is not None and fore_name is not None:\n                        article[\"authors\"].append(f\"{fore_name.text} {last_name.text}\")\n                article[\"abstract\"] = medline_citation.find(\"Article/Abstract/AbstractText\")\n                if article[\"abstract\"] is not None:\n                    article[\"abstract\"] = article[\"abstract\"].text\n                articles.append(article)\n        return articles\n    else:\n        print(\"Error fetching article details.\")\n        return []\n        \n\ndef fetch_content(pmid):\n    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/?api_key={secret_value_1}\"\n    headers = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n    }\n\n    response = requests.get(url, headers=headers)\n    time.sleep(1)\n    if response.status_code == 200:\n        #print(f\"fetching full text for PMID: {pmid}\")\n        return response.content\n    else:\n        return None","metadata":{"_uuid":"fadfac19-7575-4a6b-965f-483ecaf94824","_cell_guid":"f97b7fca-2dce-409c-9eed-4bee20b8cf0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.852883Z","iopub.execute_input":"2024-11-30T21:29:13.853491Z","iopub.status.idle":"2024-11-30T21:29:13.870898Z","shell.execute_reply.started":"2024-11-30T21:29:13.853440Z","shell.execute_reply":"2024-11-30T21:29:13.869249Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_article_sections(query, max_results):\n    # Fetch the PMIDs\n    pmids, webenv, query_key = fetch_pmid_list(query, max_results=max_results)\n    \n    # Fetch article details\n    articles = fetch_article_details(pmids, webenv, query_key)\n    \n    # Loop through each article, fetch content, and extract sections\n    for article in articles:\n        pmid = article.get('pmid')\n        if pmid:\n            html_content = fetch_content(pmid)\n            soup = BeautifulSoup(html_content, 'html.parser')\n            paragraphs = soup.find_all('p')\n            if paragraphs:\n                for para in paragraphs:\n                    para_content = para.text.strip()\n                    if para_content.startswith('Introduction'):\n                        article.update({'Introduction': para_content})\n                    elif para_content.startswith('Clinical case'):\n                        article.update({'Clinical case': para_content})\n                    elif para_content.startswith('Methods'):\n                        article.update({'Methods': para_content})\n                    elif para_content.startswith('Results'):\n                        article.update({'Results': para_content})\n                    elif para_content.startswith('Conclusion'):\n                        article.update({'Conclusion': para_content})\n    \n    df = pd.DataFrame(articles)\n    return df","metadata":{"_uuid":"315f9979-589f-44fe-b9d3-e88c94ef42c2","_cell_guid":"24f4ef2f-7652-4a02-a718-0caec05ed22a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.872813Z","iopub.execute_input":"2024-11-30T21:29:13.873326Z","iopub.status.idle":"2024-11-30T21:29:13.889086Z","shell.execute_reply.started":"2024-11-30T21:29:13.873278Z","shell.execute_reply":"2024-11-30T21:29:13.887752Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Function to get the transcript of a video\ndef get_transcript(video_id):\n    try:\n        trnscrpt = YouTubeTranscriptApi.get_transcript(video_id)\n        text = \" \".join([entry['text'] for entry in trnscrpt])\n        #print(f\"Fetched transcript\")\n        return text\n    except Exception as e:\n        print(f\"Error fetching transcript for video {video_id}: No transcript available for video ID: {video_id}.\\n\")\n        return None","metadata":{"_uuid":"0b2d7cd9-6505-4ed6-bd4a-35c86d23851f","_cell_guid":"f3aa4465-2537-4dc1-9eb7-9c497474aafa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.890573Z","iopub.execute_input":"2024-11-30T21:29:13.891040Z","iopub.status.idle":"2024-11-30T21:29:13.908063Z","shell.execute_reply.started":"2024-11-30T21:29:13.890984Z","shell.execute_reply":"2024-11-30T21:29:13.906537Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Function to process transcripts and find claims with fact-checking keywords\ndef find_top_claims_and_keywords(video):\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    \n    title = video['title']\n    description_snippet = video.get('descriptionSnippet', [])\n    description = \"\".join([desc['text'] for desc in description_snippet]) if isinstance(description_snippet, list) else \"\"\n    video_link = video['link']\n    video_id = video['id']\n\n    print(f\"Processing video: {title} (ID: {video_id})\")\n\n    # Get transcript for the video\n    transcript = get_transcript(video_id)  # Replace with your transcript-fetching function\n    if not transcript:\n        print(f\"No transcript found for video: {title}\")\n        return None\n\n    # Split the transcript into chunks of max 1000 characters (you can adjust this size)\n    max_chunk_size = 50000\n    chunks = [transcript[i:i + max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n    \n    claims_list = []\n\n    # Process each chunk\n    for i, chunk in enumerate(chunks):\n        #print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n        #print(\"Generating claims from transcript...\")\n        # Create the claims prompt for each chunk\n        claims_prompt = (\n            f\"Extract up to 3 unique, health-related, evidence-based claims from the following transcript chunk.:\\n\\n{chunk}\"\n        )\n\n        # Generate claims for this chunk\n        claims_response = model.generate_content([claims_prompt])\n        claims_text = claims_response.text.strip()\n\n        if claims_text:\n            claims_list.append(claims_text)\n        else:\n            print(f\"No claims found in chunk {i + 1}\")\n\n    # Combine the claims from all chunks\n    all_claims = \"\\n\".join(claims_list)\n    #claims_prompt = (\n        #f\"Extract up to 3 unique, evidence-based claims from the transcript below. \"\n        #f\"Exclude general knowledge or common sense:\\n\\n{transcript[:1000]}\"\n    #)\n    \n    #claims_response = model.generate_content([claims_prompt])\n\n    # Check if claims are available\n    #claims_text = claims_response.text.strip()\n    #if not claims_text:\n        #print(\"No claims generated\")\n        #return {\"title\": title, \"link\": video_link, \"claims\": [], \"keywords\": []}\n\n\n    #print(\"Extracting claims from response...\")\n    pattern = r\"\\d+\\.\\s\\*\\*(.*?)\\*\\*\\s*(.*?)(?=\\n\\d+\\.|\\Z)\"\n    claims = re.findall(pattern, all_claims, re.DOTALL)    \n    claims_list = [f\"{claim[0]} {claim[1]}\" for claim in claims]\n    #print(f\"Found {len(claims_list)} claims.\")\n\n    keyword_dict = {}\n    for i, claim in enumerate(claims_list):\n        time.sleep(1)\n        \n        # Extract keywords for fact-checking each claim\n        #print(f\"Generating keywords for Claim {i}\")\n        keywords_prompt = (\n            f\"Identify up to 3 highly relevant and specific keywords or key phrases that are essential for \"\n            f\"accurately fact-checking the following claim:\\n\\n{claims}\\n\\nFocus on the following: \\n\"\n            f\"1. Terms that directly address the core subject of the claim.\\n\"\n            f\"2. Phrases that are likely to lead to reliable and precise information when searching in academic or scientific sources.\\n\"\n            f\"3. Keywords that would yield articles, studies, or data relevant to the claim, especially those related to \"\n            f\"the topic's evidence, expert opinions, and public health implications.\\n\\n\"\n            f\"Ensure the selected keywords are both specific and comprehensive for effective fact-checking.\"\n        )\n        \n        keywords_response = model.generate_content([keywords_prompt])\n\n        keywords_text = keywords_response.text.strip()\n        keywords = re.findall(r'\"([^\"]+)\"', keywords_text)\n        keywords_string = ', '.join(keywords)\n        keyword_dict[claim] = keywords_string\n        #print(claim)\n        #print(keywords_string)\n    \n    # Display results\n    if len(claims_list)>0:\n        display(Markdown(\n            f\"### {title}\\n\"\n            f\"[Watch Video]({video_link})\\n\\n\"\n            f\"**Top Claims:**\\n\\n\" +\n            \"\\n\\n\".join([f\"{idx + 1}. {claim}\" for idx, claim in enumerate(claims_list)]) \n        ))\n    else:\n        print(\"No claim found.\")\n   \n    return {\n        \"title\": title,\n        \"link\": video_link,\n        \"claims\": claims_text,\n        \"keywords\": keyword_dict,\n        'claim_list': claims_list\n    }","metadata":{"_uuid":"d4a6c47e-70ec-42d1-91ea-a248015a883a","_cell_guid":"d3cd6fb7-cfb0-4b21-a04d-e4b8202e5fb5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.910182Z","iopub.execute_input":"2024-11-30T21:29:13.910686Z","iopub.status.idle":"2024-11-30T21:29:13.929049Z","shell.execute_reply.started":"2024-11-30T21:29:13.910631Z","shell.execute_reply":"2024-11-30T21:29:13.927826Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to perform fact-checking using Gemini AI\ndef fact_check_claims_with_confidence(articles_df, claim):\n    # Initialize a Gemini model (make sure you have the appropriate API and model)\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    # Initialize a dictionary to store the fact-check results\n    fact_check_results = {}\n\n    # Extract relevant sections from the articles (e.g., abstract, results, and conclusion)\n    articles_text = \"\"\n    for _, row in articles_df.iterrows():\n        article_content = (\n            f\"PMID: {row.get('pmid', 'N/A')}\\n\"\n            f\"Title: {row.get('title', 'N/A')}\\n\"\n            f\"Abstract: {row.get('abstract', 'N/A')}\\n\"\n            f\"Methods: {row.get('Methods', 'N/A')}\\n\"\n            f\"Results: {row.get('Results', 'N/A')}\\n\"\n            f\"Conclusion: {row.get('Conclusion', 'N/A')}\\n\"\n        )\n        articles_text += article_content + \"\\n\"\n\n    # Prepare the prompt for Gemini AI to fact-check the claim based on articles\n    time.sleep(1)\n    \n    prompt = f\"\"\"\n    Fact-check the following claim based on web data and provided articles. \n    Provide the fact-check result (True/False/Not able to validate/Conflicting results reported) \n    and a confidence score between 0 and 1:\n\n    Claim: '{claim}'\n\n    Articles:\n    {articles_text}\n\n    Please respond with:\n    1. The fact-check result: True/False/Not able to validate/Conflicting results reported\n    2. The confidence score: A numerical value between 0 and 1\n    \"\"\"\n        \n    # Generate response from Gemini AI\n    response = model.generate_content([prompt])\n\n    # Extract the fact-checking result and confidence score from the response text\n    result_text = response.text.strip()\n    print(f\"\\n ### Fact check results for claim: {claim[:claim.find(':')]}.\")\n    print(f\"### Fact Check Analysis:\\n{result_text}\\n\")\n\n\n    try:\n        # Try to parse the fact-check result and confidence score from the response\n        fact_check_result = result_text.split(\",\")[0].split(\":\")[1].strip()\n        confidence_score = float(result_text.split(\",\")[1].split(\":\")[1].strip())\n\n        # Store the results in a dictionary\n        fact_check_results[claim] = {'result': fact_check_result, 'confidence_score': confidence_score}\n\n    except (IndexError, ValueError) as e:\n        # Handle cases where the response format is not as expected\n        fact_check_results[claim] = {'result': 'Error', 'confidence_score': 0.0, 'error': str(e)}\n\n    return fact_check_results","metadata":{"_uuid":"e7c1a31d-9011-4d3d-9fbf-3be17cd4519f","_cell_guid":"1987a2c5-ff0e-4c9d-a8f8-9819c06b2211","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.930771Z","iopub.execute_input":"2024-11-30T21:29:13.931348Z","iopub.status.idle":"2024-11-30T21:29:13.953965Z","shell.execute_reply.started":"2024-11-30T21:29:13.931284Z","shell.execute_reply":"2024-11-30T21:29:13.952342Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def search_academic_articles(keywords):\n    query = \"+\".join(keywords)\n    url = f\"https://api.semanticscholar.org/v1/paper/search?query={query}&limit=5\"\n    response = requests.get(url)\n    articles = response.json()\n    return articles['data'] if 'data' in articles else []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:29:13.956657Z","iopub.execute_input":"2024-11-30T21:29:13.957839Z","iopub.status.idle":"2024-11-30T21:29:13.972762Z","shell.execute_reply.started":"2024-11-30T21:29:13.957781Z","shell.execute_reply":"2024-11-30T21:29:13.971494Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"search = VideosSearch('health', limit=1)  # Adjust the limit as needed\nresults = search.result()\n\n\n# Check each result for claim-related keywords in the title, description, or transcript\nfor video in results['result']:\n    claim_results = find_top_claims_and_keywords(video)\n    if claim_results:\n        for claim in claim_results['claim_list']:\n            articles_df = pd.DataFrame()\n            if claim_results['keywords'][claim]:\n                keywords = claim_results['keywords'][claim]\n                for item in keywords.split(','):\n                    new_articles_df = extract_article_sections(query=item, max_results=20)\n                    articles_df = pd.concat([articles_df, new_articles_df])\n                    if len(new_articles_df)>0:\n                        print(f\"Fetched {len(new_articles_df)} articles with keywrods: {item}.\")\n                if len(articles_df)>0:\n                    fact_check_results = fact_check_claims_with_confidence(articles_df, claim)\n            \n                else:\n                    print(f\"No articles found for the claim with keywords: {claim_results['keywords'][claim]}.\\n\")\n                    search_academic_articles(keywords)","metadata":{"_uuid":"3b65ab6a-c40b-43c6-b0b9-915b0cd41f4a","_cell_guid":"5908cc2e-95a0-4daf-9622-f0e934db91fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T21:29:13.975644Z","iopub.execute_input":"2024-11-30T21:29:13.976020Z"}},"outputs":[{"name":"stdout","text":"Processing video: HEALTH - Full Performance (Live on KEXP) (ID: 0kYJ3ECZk7c)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### HEALTH - Full Performance (Live on KEXP)\n[Watch Video](https://www.youtube.com/watch?v=0kYJ3ECZk7c)\n\n**Top Claims:**\n\n"},"metadata":{}},{"name":"stdout","text":"Processing video: America’s Health Crisis EXPOSED - Why Toxic Food Industry FEARS RFK Jr. (ID: vfI5xQo7XiY)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### America’s Health Crisis EXPOSED - Why Toxic Food Industry FEARS RFK Jr.\n[Watch Video](https://www.youtube.com/watch?v=vfI5xQo7XiY)\n\n**Top Claims:**\n\n1. Higher healthcare spending, lower life expectancy: The US spends a significantly larger percentage of its GDP on healthcare than other developed nations (17.8% vs. lower percentages in countries like Germany, France, and Korea), yet has a lower life expectancy.  This suggests inefficiencies and issues within the US healthcare system impacting overall population health outcomes.\n\n\n2. High rates of preventable deaths: The US has a substantially higher rate of avoidable deaths (due to factors like smoking, excessive alcohol consumption, and poor diet) compared to other developed nations. This highlights a public health crisis related to lifestyle choices and potentially inadequate preventative measures.\n\n\n3. High infant and maternal mortality rates: The US exhibits significantly higher rates of infant and maternal mortality than the OECD average. This points to serious problems in maternal and child healthcare access and quality, a stark contrast to the high level of healthcare spending."},"metadata":{}},{"name":"stdout","text":"Fetched 20 articles with keywrods:  Preventable mortality rates international comparison.\nFetched 40 articles with keywrods:  preventable mortality.\nFetched 60 articles with keywrods:  US maternal and infant mortality rates disparities.\nFetched 80 articles with keywrods:  maternal and infant mortality rates.\nfact check results for claim: Higher healthcare spending, lower life expectancy.\n1. **Fact-check result:** True\n\n2. **Confidence score:** 0.8\n\n\n**Justification:**\n\nThe claim states that the US spends a significantly larger percentage of its GDP on healthcare than other developed nations, yet has a lower life expectancy.  This discrepancy suggests inefficiencies within the US healthcare system.\n\nThe provided articles don't directly address US healthcare spending as a percentage of GDP or make direct comparisons of life expectancy across multiple developed nations. However, many articles touch upon the topic of mortality and morbidity within specific groups in the US population, highlighting various factors contributing to health disparities.  These factors — ranging from access to healthcare, social determinants of health, and systemic issues— indirectly support the claim's assertion of inefficiencies.  The high mortality rates and health disparities demonstrated in many studies strongly suggest that despite high healthcare spending, the system isn't effectively translating expenditure into improved population-level health outcomes.  Therefore, the claim is considered **True**, albeit with a confidence score below 1 because the articles don't contain the precise GDP spending data and international life expectancy comparisons needed for complete verification.  The implicit evidence in the articles, however, supports the core argument.\nFetched 100 articles with keywrods:  Preventable mortality rates international comparison.\nFetched 120 articles with keywrods:  preventable mortality.\nFetched 140 articles with keywrods:  international comparison.\nFetched 160 articles with keywrods:  US maternal and infant mortality rates disparities.\nFetched 180 articles with keywrods:  maternal.\nFetched 200 articles with keywrods:  infant mortality.\nFetched 220 articles with keywrods:  disparities.\nfact check results for claim: High rates of preventable deaths.\n1. **Fact-check result:** True\n\n2. **Confidence score:** 0.8\n\n\nSeveral of the provided articles support the claim that the US has higher rates of preventable deaths compared to other developed nations, although not all directly address the specific factors mentioned (smoking, excessive alcohol, poor diet).  The articles demonstrate higher mortality rates in the US from various causes, often linked to social determinants of health and lifestyle choices.  While there isn't a single article providing a direct comparison of preventable death rates across multiple developed nations using precisely those three factors, the available data strongly suggests the claim is accurate. The confidence score is not 1.0 because the provided papers don't offer a direct, explicit comparison across all listed factors, relying on inferences from data on mortality from various related conditions.\nFetched 221 articles with keywrods:  Preventable mortality rates US vs OECD.\n","output_type":"stream"}],"execution_count":null}]}