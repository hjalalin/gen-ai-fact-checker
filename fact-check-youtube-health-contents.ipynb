{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10055796,"sourceType":"datasetVersion","datasetId":6196261}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport time\nimport os\nimport requests\nimport xml.etree.ElementTree as ET\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom IPython.display import Markdown, display, YouTubeVideo\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom google.auth.credentials import AnonymousCredentials\nfrom google.auth import compute_engine\nfrom IPython.display import Markdown, display\nfrom kaggle_secrets import UserSecretsClient\nfrom nltk.corpus import wordnet\n\n! pip install -q youtube-search-python \n!pip install youtube-transcript-api \nfrom youtubesearchpython import VideosSearch\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom youtube_transcript_api.formatters import JSONFormatter","metadata":{"_uuid":"40e0500a-3904-4173-b4a5-d7976be33f68","_cell_guid":"36457369-65b6-4d8b-9052-6c5f5e06e075","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:04:54.623804Z","iopub.execute_input":"2024-11-30T22:04:54.624195Z","iopub.status.idle":"2024-11-30T22:05:14.789511Z","shell.execute_reply.started":"2024-11-30T22:04:54.624161Z","shell.execute_reply":"2024-11-30T22:05:14.787828Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: youtube-transcript-api in /opt/conda/lib/python3.10/site-packages (0.6.3)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (0.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2024.8.30)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def mock_get_universe_domain(request):\n    return \"googleapis.com\"\n\n# Override the original metadata fetching function\ncompute_engine._metadata.get_universe_domain = mock_get_universe_domain\n\n# Set the Google application credentials manually (replace with the actual path)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/google-cloud-key/esoteric-cab-443306-n6-8b6ccf376c34.json\"","metadata":{"_uuid":"1b9f114b-9313-4821-a216-d24aa7ae7890","_cell_guid":"5b1831eb-9266-487e-b77b-7b0fabc8e1f0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:14.792918Z","iopub.execute_input":"2024-11-30T22:05:14.793931Z","iopub.status.idle":"2024-11-30T22:05:14.801594Z","shell.execute_reply.started":"2024-11-30T22:05:14.793872Z","shell.execute_reply":"2024-11-30T22:05:14.800419Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GEMINI-API-KEY\")\nsecret_value_1 = user_secrets.get_secret(\"ncbi_api_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:05:14.802844Z","iopub.execute_input":"2024-11-30T22:05:14.803197Z","iopub.status.idle":"2024-11-30T22:05:15.418071Z","shell.execute_reply.started":"2024-11-30T22:05:14.803154Z","shell.execute_reply":"2024-11-30T22:05:15.416935Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def fetch_pmid_list(query, max_results=100):\n    base_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key={secret_value_1}\"\n    params = {\n        \"db\": \"pubmed\",\n        \"term\": query,\n        \"retmax\": max_results,\n        \"usehistory\": \"y\"\n    }\n    response = requests.get(base_url, params=params)\n    time.sleep(1)\n    if response.status_code == 200:\n        root = ET.fromstring(response.content)\n        webenv = root.find(\"WebEnv\").text\n        query_key = root.find(\"QueryKey\").text\n        pmids = [id.text for id in root.findall(\"IdList/Id\")]\n        return pmids, webenv, query_key\n    else:\n        print(\"Error fetching PMIDs.\")\n        return [], None, None\n        \n\ndef fetch_article_details(pmids, webenv, query_key, retstart=0, retmax=100):\n    base_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key={secret_value_1}\"\n    ids = \",\".join(pmids)\n    params = {\n        \"db\": \"pubmed\",\n        \"id\": ids,\n        \"retstart\": retstart,\n        \"retmax\": retmax,\n        \"WebEnv\": webenv,\n        \"query_key\": query_key,\n        \"rettype\": \"xml\",\n        \"retmode\": \"xml\"\n    }\n    response = requests.get(base_url, params=params)\n    time.sleep(1)\n    if response.status_code == 200:\n        root = ET.fromstring(response.content)\n        articles = []\n        for docsum in root.findall(\"PubmedArticle\"):\n            article = {}\n            medline_citation = docsum.find(\"MedlineCitation\")\n            if medline_citation is not None:\n                article[\"pmid\"] = medline_citation.find(\"PMID\").text\n                article[\"title\"] = medline_citation.find(\"Article/ArticleTitle\").text\n                article[\"source\"] = medline_citation.find(\"Article/Journal/Title\").text\n                article[\"authors\"] = []\n                for author in medline_citation.findall(\"Article/AuthorList/Author\"):\n                    last_name = author.find(\"LastName\")\n                    fore_name = author.find(\"ForeName\")\n                    if last_name is not None and fore_name is not None:\n                        article[\"authors\"].append(f\"{fore_name.text} {last_name.text}\")\n                article[\"abstract\"] = medline_citation.find(\"Article/Abstract/AbstractText\")\n                if article[\"abstract\"] is not None:\n                    article[\"abstract\"] = article[\"abstract\"].text\n                articles.append(article)\n        return articles\n    else:\n        print(\"Error fetching article details.\")\n        return []\n        \n\ndef fetch_content(pmid):\n    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/?api_key={secret_value_1}\"\n    headers = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n    }\n\n    response = requests.get(url, headers=headers)\n    time.sleep(1)\n    if response.status_code == 200:\n        #print(f\"fetching full text for PMID: {pmid}\")\n        return response.content\n    else:\n        return None","metadata":{"_uuid":"fadfac19-7575-4a6b-965f-483ecaf94824","_cell_guid":"f97b7fca-2dce-409c-9eed-4bee20b8cf0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.419579Z","iopub.execute_input":"2024-11-30T22:05:15.419950Z","iopub.status.idle":"2024-11-30T22:05:15.434219Z","shell.execute_reply.started":"2024-11-30T22:05:15.419915Z","shell.execute_reply":"2024-11-30T22:05:15.433071Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def extract_article_sections(query, max_results):\n    # Fetch the PMIDs\n    pmids, webenv, query_key = fetch_pmid_list(query, max_results=max_results)\n    \n    # Fetch article details\n    articles = fetch_article_details(pmids, webenv, query_key)\n    \n    # Loop through each article, fetch content, and extract sections\n    for article in articles:\n        pmid = article.get('pmid')\n        if pmid:\n            html_content = fetch_content(pmid)\n            soup = BeautifulSoup(html_content, 'html.parser')\n            paragraphs = soup.find_all('p')\n            if paragraphs:\n                for para in paragraphs:\n                    para_content = para.text.strip()\n                    if para_content.startswith('Introduction'):\n                        article.update({'Introduction': para_content})\n                    elif para_content.startswith('Clinical case'):\n                        article.update({'Clinical case': para_content})\n                    elif para_content.startswith('Methods'):\n                        article.update({'Methods': para_content})\n                    elif para_content.startswith('Results'):\n                        article.update({'Results': para_content})\n                    elif para_content.startswith('Conclusion'):\n                        article.update({'Conclusion': para_content})\n    \n    df = pd.DataFrame(articles)\n    return df","metadata":{"_uuid":"315f9979-589f-44fe-b9d3-e88c94ef42c2","_cell_guid":"24f4ef2f-7652-4a02-a718-0caec05ed22a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.436549Z","iopub.execute_input":"2024-11-30T22:05:15.436911Z","iopub.status.idle":"2024-11-30T22:05:15.453174Z","shell.execute_reply.started":"2024-11-30T22:05:15.436880Z","shell.execute_reply":"2024-11-30T22:05:15.451750Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Function to get the transcript of a video\ndef get_transcript(video_id):\n    try:\n        trnscrpt = YouTubeTranscriptApi.get_transcript(video_id)\n        text = \" \".join([entry['text'] for entry in trnscrpt])\n        #print(f\"Fetched transcript\")\n        return text\n    except Exception as e:\n        print(f\"Error fetching transcript for video {video_id}: No transcript available for video ID: {video_id}.\\n\")\n        return None","metadata":{"_uuid":"0b2d7cd9-6505-4ed6-bd4a-35c86d23851f","_cell_guid":"f3aa4465-2537-4dc1-9eb7-9c497474aafa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.454705Z","iopub.execute_input":"2024-11-30T22:05:15.455059Z","iopub.status.idle":"2024-11-30T22:05:15.475383Z","shell.execute_reply.started":"2024-11-30T22:05:15.455025Z","shell.execute_reply":"2024-11-30T22:05:15.474097Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Function to process transcripts and find claims with fact-checking keywords\ndef find_top_claims_and_keywords(video):\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    \n    title = video['title']\n    description_snippet = video.get('descriptionSnippet', [])\n    description = \"\".join([desc['text'] for desc in description_snippet]) if isinstance(description_snippet, list) else \"\"\n    video_link = video['link']\n    video_id = video['id']\n\n    print(f\"Processing video: {title} (ID: {video_id})\")\n\n    # Get transcript for the video\n    transcript = get_transcript(video_id)  # Replace with your transcript-fetching function\n    if not transcript:\n        print(f\"No transcript found for video: {title}\")\n        return None\n\n    # Split the transcript into chunks of max 1000 characters (you can adjust this size)\n    max_chunk_size = 50000\n    chunks = [transcript[i:i + max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n    \n    claims_list = []\n\n    # Process each chunk\n    for i, chunk in enumerate(chunks):\n        #print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n        #print(\"Generating claims from transcript...\")\n        # Create the claims prompt for each chunk\n        claims_prompt = (\n            f\"Extract up to 3 unique, health-related, evidence-based claims from the following transcript chunk.:\\n\\n{chunk}\"\n        )\n\n        # Generate claims for this chunk\n        claims_response = model.generate_content([claims_prompt])\n        claims_text = claims_response.text.strip()\n\n        if claims_text:\n            claims_list.append(claims_text)\n        else:\n            print(f\"No claims found in chunk {i + 1}\")\n\n    # Combine the claims from all chunks\n    all_claims = \"\\n\".join(claims_list)\n    #claims_prompt = (\n        #f\"Extract up to 3 unique, evidence-based claims from the transcript below. \"\n        #f\"Exclude general knowledge or common sense:\\n\\n{transcript[:1000]}\"\n    #)\n    \n    #claims_response = model.generate_content([claims_prompt])\n\n    # Check if claims are available\n    #claims_text = claims_response.text.strip()\n    #if not claims_text:\n        #print(\"No claims generated\")\n        #return {\"title\": title, \"link\": video_link, \"claims\": [], \"keywords\": []}\n\n\n    #print(\"Extracting claims from response...\")\n    pattern = r\"\\d+\\.\\s\\*\\*(.*?)\\*\\*\\s*(.*?)(?=\\n\\d+\\.|\\Z)\"\n    claims = re.findall(pattern, all_claims, re.DOTALL)    \n    claims_list = [f\"{claim[0]} {claim[1]}\" for claim in claims]\n    #print(f\"Found {len(claims_list)} claims.\")\n\n    keyword_dict = {}\n    for i, claim in enumerate(claims_list):\n        time.sleep(1)\n        \n        # Extract keywords for fact-checking each claim\n        #print(f\"Generating keywords for Claim {i}\")\n        keywords_prompt = (\n            f\"Identify up to 3 highly relevant and specific keywords or key phrases that are essential for \"\n            f\"accurately fact-checking the following claim:\\n\\n{claims}\\n\\nFocus on the following: \\n\"\n            f\"1. Terms that directly address the core subject of the claim.\\n\"\n            f\"2. Phrases that are likely to lead to reliable and precise information when searching in academic or scientific sources.\\n\"\n            f\"3. Keywords that would yield articles, studies, or data relevant to the claim, especially those related to \"\n            f\"the topic's evidence, expert opinions, and public health implications.\\n\\n\"\n            f\"Ensure the selected keywords are both specific and comprehensive for effective fact-checking.\"\n        )\n        \n        keywords_response = model.generate_content([keywords_prompt])\n\n        keywords_text = keywords_response.text.strip()\n        keywords = re.findall(r'\"([^\"]+)\"', keywords_text)\n        keywords_string = ', '.join(keywords)\n        keyword_dict[claim] = keywords_string\n        #print(claim)\n        #print(keywords_string)\n    \n    # Display results\n    if len(claims_list)>0:\n        display(Markdown(\n            f\"### {title}\\n\"\n            f\"[Watch Video]({video_link})\\n\\n\"\n            f\"**Top Claims:**\\n\\n\" +\n            \"\\n\\n\".join([f\"{idx + 1}. {claim}\" for idx, claim in enumerate(claims_list)]) \n        ))\n    else:\n        print(\"No claim found.\")\n   \n    return {\n        \"title\": title,\n        \"link\": video_link,\n        \"claims\": claims_text,\n        \"keywords\": keyword_dict,\n        'claim_list': claims_list\n    }","metadata":{"_uuid":"d4a6c47e-70ec-42d1-91ea-a248015a883a","_cell_guid":"d3cd6fb7-cfb0-4b21-a04d-e4b8202e5fb5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.477418Z","iopub.execute_input":"2024-11-30T22:05:15.477818Z","iopub.status.idle":"2024-11-30T22:05:15.493442Z","shell.execute_reply.started":"2024-11-30T22:05:15.477783Z","shell.execute_reply":"2024-11-30T22:05:15.492353Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Function to perform fact-checking using Gemini AI\ndef fact_check_claims_with_confidence(articles_df, claim):\n    # Initialize a Gemini model (make sure you have the appropriate API and model)\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    # Initialize a dictionary to store the fact-check results\n    fact_check_results = {}\n\n    # Extract relevant sections from the articles (e.g., abstract, results, and conclusion)\n    articles_text = \"\"\n    for _, row in articles_df.iterrows():\n        article_content = (\n            f\"PMID: {row.get('pmid', 'N/A')}\\n\"\n            f\"Title: {row.get('title', 'N/A')}\\n\"\n            f\"Abstract: {row.get('abstract', 'N/A')}\\n\"\n            f\"Methods: {row.get('Methods', 'N/A')}\\n\"\n            f\"Results: {row.get('Results', 'N/A')}\\n\"\n            f\"Conclusion: {row.get('Conclusion', 'N/A')}\\n\"\n        )\n        articles_text += article_content + \"\\n\"\n\n    # Prepare the prompt for Gemini AI to fact-check the claim based on articles\n    time.sleep(1)\n    \n    prompt = f\"\"\"\n    Fact-check the following claim based on web data and provided articles. \n    Provide the fact-check result (True/False/Not able to validate/Conflicting results reported) \n    and a confidence score between 0 and 1:\n\n    Claim: '{claim}'\n\n    Articles:\n    {articles_text}\n\n    Please respond with:\n    1. The fact-check result: True/False/Not able to validate/Conflicting results reported\n    2. The confidence score: A numerical value between 0 and 1\n    \"\"\"\n        \n    # Generate response from Gemini AI\n    response = model.generate_content([prompt])\n\n    # Extract the fact-checking result and confidence score from the response text\n    result_text = response.text.strip()\n    print(f\"\\n ### Fact check results for claim: {claim[:claim.find(':')]}.\")\n    print(f\"### Fact Check Analysis:\\n{result_text}\\n\")\n\n\n    try:\n        # Try to parse the fact-check result and confidence score from the response\n        fact_check_result = result_text.split(\",\")[0].split(\":\")[1].strip()\n        confidence_score = float(result_text.split(\",\")[1].split(\":\")[1].strip())\n\n        # Store the results in a dictionary\n        fact_check_results[claim] = {'result': fact_check_result, 'confidence_score': confidence_score}\n\n    except (IndexError, ValueError) as e:\n        # Handle cases where the response format is not as expected\n        fact_check_results[claim] = {'result': 'Error', 'confidence_score': 0.0, 'error': str(e)}\n\n    return fact_check_results","metadata":{"_uuid":"e7c1a31d-9011-4d3d-9fbf-3be17cd4519f","_cell_guid":"1987a2c5-ff0e-4c9d-a8f8-9819c06b2211","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.495267Z","iopub.execute_input":"2024-11-30T22:05:15.495590Z","iopub.status.idle":"2024-11-30T22:05:15.513059Z","shell.execute_reply.started":"2024-11-30T22:05:15.495559Z","shell.execute_reply":"2024-11-30T22:05:15.511897Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def search_academic_articles(keywords):\n    query = \"+\".join(keywords)\n    url = f\"https://api.semanticscholar.org/v1/paper/search?query={query}&limit=5\"\n    response = requests.get(url)\n    articles = response.json()\n    return articles['data'] if 'data' in articles else []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:05:15.514534Z","iopub.execute_input":"2024-11-30T22:05:15.514875Z","iopub.status.idle":"2024-11-30T22:05:15.533279Z","shell.execute_reply.started":"2024-11-30T22:05:15.514841Z","shell.execute_reply":"2024-11-30T22:05:15.531730Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"search = VideosSearch('health', limit=1)  # Adjust the limit as needed\nresults = search.result()\n\n\n# Check each result for claim-related keywords in the title, description, or transcript\nfor video in results['result']:\n    claim_results = find_top_claims_and_keywords(video)\n    if claim_results:\n        for claim in claim_results['claim_list']:\n            articles_df = pd.DataFrame()\n            if claim_results['keywords'][claim]:\n                keywords = claim_results['keywords'][claim]\n                for item in keywords.split(','):\n                    if len(item)>0:\n                        new_articles_df = extract_article_sections(query=item, max_results=20)\n                        articles_df = pd.concat([articles_df, new_articles_df])\n                        if len(new_articles_df)>0:\n                            print(f\"Fetched {len(new_articles_df)} articles with keywrods: {item}.\")\n                if len(articles_df)>0:\n                    fact_check_results = fact_check_claims_with_confidence(articles_df, claim)\n            \n                else:\n                    print(f\"No articles found for the claim with keywords: {claim_results['keywords'][claim]}.\\n\")\n                    search_academic_articles(keywords)","metadata":{"_uuid":"3b65ab6a-c40b-43c6-b0b9-915b0cd41f4a","_cell_guid":"5908cc2e-95a0-4daf-9622-f0e934db91fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-30T22:05:15.534705Z","iopub.execute_input":"2024-11-30T22:05:15.535182Z","iopub.status.idle":"2024-11-30T22:07:30.226919Z","shell.execute_reply.started":"2024-11-30T22:05:15.535114Z","shell.execute_reply":"2024-11-30T22:07:30.225736Z"}},"outputs":[{"name":"stdout","text":"Processing video: America’s Health Crisis EXPOSED - Why Toxic Food Industry FEARS RFK Jr. (ID: vfI5xQo7XiY)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### America’s Health Crisis EXPOSED - Why Toxic Food Industry FEARS RFK Jr.\n[Watch Video](https://www.youtube.com/watch?v=vfI5xQo7XiY)\n\n**Top Claims:**\n\n1. Higher US healthcare spending correlates with lower life expectancy: The transcript highlights that the US spends a significantly higher percentage of its GDP on healthcare than other countries, yet has a lower life expectancy. This is a widely documented fact supported by various international health data comparisons (e.g., from the OECD, WHO).\n\n\n2. Higher rates of avoidable deaths in the US: The transcript points to the US having a substantially higher rate of avoidable deaths (due to factors like smoking, excessive alcohol consumption, and poor diet) compared to peer nations.  This claim can be verified through mortality data analyzing causes of death and comparing across countries.\n\n\n3. High US rates of infant and maternal mortality: The transcript cites significantly higher rates of infant and maternal mortality in the US compared to OECD averages. This is a well-established disparity supported by data from organizations like the CDC and WHO."},"metadata":{}},{"name":"stdout","text":"Fetched 20 articles with keywrods: avoidable.\n\n ### Fact check results for claim: Higher US healthcare spending correlates with lower life expectancy.\n### Fact Check Analysis:\n1. **Fact-check result:** True\n\n2. **Confidence score:** 0.8\n\n**Explanation:**\n\nThe provided articles do not directly address the correlation between US healthcare spending and life expectancy.  However, the claim itself is widely supported by numerous readily available data sources from organizations like the OECD and WHO.  These sources consistently show that the US spends far more on healthcare per capita and as a percentage of GDP than many other developed nations, yet its life expectancy is comparatively lower.  Therefore, while the articles are irrelevant, the claim's veracity is easily verifiable through publicly available data, making \"True\" the appropriate fact-check result.  The confidence score is not 1.0 because the supporting evidence is not directly within the provided text; it relies on external, widely-accepted datasets.\n\nFetched 20 articles with keywrods:  Avoidable mortality rates international comparison.\nFetched 20 articles with keywrods:  avoidable mortality.\n\n ### Fact check results for claim: Higher rates of avoidable deaths in the US.\n### Fact Check Analysis:\n1. **Fact-check result:** True\n\n2. **Confidence score:** 0.8\n\n**Explanation:**\n\nPMID: 37303714 (\"Missing Americans\") directly supports the claim.  This study shows a substantially higher rate of avoidable deaths in the US compared to 21 other wealthy nations,  attributing the excess deaths to factors like those mentioned in the claim (although it doesn't explicitly break them down into smoking, alcohol, and diet). The large difference in avoidable mortality rates between the US and peer nations provides strong evidence for the claim's accuracy.  While the other articles are relevant to mortality and healthcare in various contexts, they don't directly address the core comparison of avoidable death rates across nations as stated in the claim.  Therefore, the confidence score is high but not 1.0 because the PMID 37303714 study doesn't explicitly list smoking, alcohol, and poor diet as the *sole* causes of the excess deaths, though they're strongly implied as significant contributors to overall avoidable mortality.\n\n","output_type":"stream"}],"execution_count":21}]}